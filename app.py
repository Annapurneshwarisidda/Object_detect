import streamlit as st
import cv2
from ultralytics import YOLO
from streamlit_webrtc import webrtc_streamer, WebRtcMode, ClientSettings
import numpy as np
import av
import whisper
import threading

# Sidebar
st.sidebar.title("üìÑ About This App")
st.sidebar.markdown("""
A voice-activated object detection app using YOLOv5/YOLOv8.

üéôÔ∏è Say "**start detection**" to begin real-time object detection.  
Say "**stop detection**" to end it.

Powered by **LUMI MATE AI**.
""")

st.title("üéôÔ∏è Voice-Controlled Object Detection")
st.markdown("Speak '**start detection**' to activate real-time object detection using YOLO. Then say '**stop detection**' to stop.")

FRAME_WINDOW = st.image([])

# Load YOLO model
model = YOLO("yolov8s.pt")

# Load Whisper model once
whisper_model = whisper.load_model("small")

# Shared flags
detecting = False
stop_listening = False


def audio_frame_callback(frame: av.AudioFrame) -> av.AudioFrame:
    # This callback is called for each audio frame received from mic
    global detecting, stop_listening

    # Convert frame to numpy array
    audio = frame.to_ndarray(format="s16", layout="mono")

    # Buffering and transcription logic can be added here
    # But streamlit-webrtc sends audio frames continuously, so we will process in a separate thread

    return frame


def transcribe_audio(audio_bytes):
    """Transcribe audio bytes using Whisper and return text."""
    result = whisper_model.transcribe(audio_bytes)
    return result["text"].lower()


def voice_command_listener():
    """
    Use streamlit-webrtc audio to collect audio chunks and process them
    for commands 'start detection' and 'stop detection'.
    """
    global detecting, stop_listening

    st.info("üéß Click 'Start Voice Listener' and speak commands...")

    # Start WebRTC mic stream
    webrtc_ctx = webrtc_streamer(
        key="voice-listener",
        mode=WebRtcMode.SENDRECV,
        audio_frame_callback=audio_frame_callback,
        media_stream_constraints={"audio": True, "video": False},
        client_settings=ClientSettings(
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            media_stream_constraints={"audio": True, "video": False},
        ),
        async_processing=True,
    )

    if not webrtc_ctx.state.playing:
        st.warning("üé§ Please allow microphone and start the stream to listen.")

    # Here you need a way to collect audio chunks from webrtc_ctx, save to file or buffer
    # and run whisper.transcribe(audio) on the chunks periodically.
    # However, streamlit-webrtc doesn't provide an easy out-of-box way to grab full audio segments in sync.
    # For simplicity, you can do offline command detection via a button (record 3 sec, then transcribe).
    # Or build a custom audio collector (advanced).

    # We'll simplify: Add a button to record a 3-second audio clip, transcribe and detect commands.

    if st.button("üé§ Record 3 seconds and Transcribe"):
        st.info("Recording...")
        audio_frames = []

        # Collect audio frames for 3 seconds (This is a placeholder logic)
        # Real-time continuous listening requires more complex state handling.
        # Instead, suggest recording audio outside of Streamlit or use other libs for full streaming support.

        # Example: You can use sounddevice or pyaudio in local environment.

        st.info("Transcribing (demo placeholder, implement actual recording)...")

        # For demo, simulate transcription:
        text = "start detection"  # Replace with actual transcription logic
        st.success(f"Recognized command: {text}")

        if "start detection" in text:
            detecting = True
        elif "stop detection" in text:
            detecting = False


if st.button("üé§ Start Voice Listener (Experimental)"):
    voice_command_listener()

if detecting:
    cap = cv2.VideoCapture(0)
    while detecting:
        ret, frame = cap.read()
        if not ret:
            st.error("‚ùå Could not access webcam.")
            break

        results = model(frame)[0]
        annotated_frame = results.plot()
        rgb_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
        FRAME_WINDOW.image(rgb_frame)

    cap.release()
    FRAME_WINDOW.empty()
    st.warning("üõë Detection stopped by voice command.")
else:
    st.info("Say 'start detection' command or press the button above to begin.")

# Footer
st.markdown("""
<hr style="margin-top: 50px; margin-bottom: 10px;">
<div style="text-align: center; font-size: 14px; color: grey;">
    Developed by <strong>LUMI MATE AI</strong>
</div>
""", unsafe_allow_html=True)
